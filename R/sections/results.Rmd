## Results

### Test of Proportions

```{r proportions_prep, echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE, eval = TRUE}
## Calculate difference in terms of cohens D
p1 <- 0.1
p2 <- 0.01
cohens.d <- ES.h(p1 = p1, p2 = p2)

```

The sample size has been calculated as 10% detection rate via intra-uternine Magentic Resonance Imaginag (iuMR) compared to 1% via Ultrasound (US).  This equates to Cohen's D of `r cohens.d`, but since this is only an estimate of the difference in detection rates it is informative to consider values around this should the estimate be inaccurate.  To that end a range of sample size calculations for varying detection rates in the iuMR group have been tested for a range of powers and three significance levels (`p = 0.05`, `p = 0.01` and `p = 0.001`).  The results are plotted below (the vertical line represents the estimated effect size of `r cohens.d`).

```{r proportions, echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE, eval = TRUE}
## Set the proportion in each arm, from Stata do-file...
##
## *Specify 10% iuMR correct US incorrect; 1% vice versa, 90%
## power pairedproportions .1 .01, power(0.9)
## *allow 25% drop out
## di r(N)/.75
p1.range <- seq(from = 0.02,
          to   = 0.12,
          by   = 0.01)
p2      <- 0.01
## Set range of parameters
sig.levels <- c(0.05, 0.01, 0.001)
power.range <- seq(from = 0.75,
             to   = 0.95,
             by   = 0.05)
## Set up a data frame to hold results
rm(results)
results <- data.frame(h           = double(),
                      n           = double(),
                      sig.level   = double(),
                      power       = double(),
                      alternative = character(),
                      method      = character(),
                      note        = character())
## Run calculations
for(power in power.range){
    for(sig.level in sig.levels){
        for(p1 in p1.range){
        ## paste0('P1 : ', p1) %>% print()
            ## paste0('P2 : ', p2) %>% print()
            ## ES.h(p1 = p1, p2 = p2) %>% print()
            t <- pwr.2p.test(h         = ES.h(p1 = p1, p2 = p2),
                             sig.level = sig.level,
                             power     = power)
            results <- rbind(results,
                             unlist(t))
        }
    }
}
names(results) <- c('h', 'n', 'sig.level', 'power', 'alternative', 'method', 'note')
results <- mutate(results,
                  h         = as.double(h),
                  n         = as.numeric(n),
                  sig.level = as.numeric(sig.level),
                  power     = as.numeric(power))
## Unadjusted sample size
n.unadjusted <- dplyr::filter(results,
                              round(h, digits = 3) == round(cohens.d, digits = 3) &
                              sig.level == 0.05 &
                              power     == 0.9)$n
n.unadjusted <- ceiling(n.unadjusted)
## Adjust n to account for 25% drop out
results <- mutate(results,
                  n         = n / 0.75)
## Plot
dplyr::filter(results, power %in% c(0.8, 0.85, 0.9, 0.95) & h > 0.4 & h < 0.5) %>%
ggplot(aes(x = h, y = n, colour = factor(power))) +
    facet_grid(sig.level ~ .) +
    ## geom_line() +
    geom_smooth(se = FALSE) +
    geom_vline(xintercept = cohens.d) +
    xlab('Cohens d') + ylab('N (per sample)') +
    ggtitle('Power for test of Proportions for different Significance Levels') +
    theme_bw() +
    scale_colour_discrete(name = 'Power')
## The sample size required for 90% power and 5% significance
n.adjusted <- dplyr::filter(results,
                              round(h, digits = 3) == round(cohens.d, digits = 3) &
                              sig.level == 0.05 &
                              power     == 0.9)$n
n.adjusted <- ceiling(n.adjusted)

```

The sample size required to detect a 10% detection rate via iuMR and 1% via US with 90% power and a significance threshold of `p = 0.05` is `r n.unadjusted` per arm, but allowing for 25% attrition (as plotted above) this increases to `r n.adjusted` per arm (`r 2 * n.adjusted` overall).

\pagebreak

This does not match that derived by [Mike Bradburn](mailto:m.bradburn@sheffield.ac.uk) (see Overview section), however I have reviewed the code written to perform the calculation and run it independently in [Stata v14.1](https://www.stata.com/) and confirm that it produces the suggested sample size used in the current version of the draft.

```
*1 is easy, done
*Specify 10% iuMR correct US incorrect; 1% vice versa, 90%
. power pairedproportions .1 .01, power(0.9)

Performing iteration ...

Estimated sample size for a two-sample paired-proportions test
Large-sample McNemar's test
Ho: p21 = p12  versus  Ha: p21 != p12

Study parameters:

        alpha =    0.0500
        power =    0.9000
        delta =   -0.0900  (difference)
          p12 =    0.1000
          p21 =    0.0100

Estimated sample size:

            N =       139

*allow 25% drop out
. di r(N)/.75

185.33333


```

Resolving the differences between the two pieces of software is beyond the scope of the Quality Control exercise.


### Ordinal Regression

Sample sizes have been derived via simulation and appear to have used an element of trial and error to determine appropriate numbers.  The approach taken has been to artificially categorise a continuous variable (cranial circumference) into three risk groups based on the observed value being more than two standard deviations from the mean.  Neither the mean, nor the standard deviation on which this assessment is being made are stated in the current draft document, nor is there an explanation of why the risk for each category has been selected (beyond evidence of some trial and error in the original calculations).

It is this authors point of view that categorising a continuous variable prior to analysing data should not be undertaken, it is well known and demonstrated that this reduces the power compared to analysing the underlying continuous variable (@kahan2016, @dawson2012, @naggara2011, @heavner2010, @vickers2008, @altman2006, @royston2006, @maccallum2002) and various other problems such as cut-points being arbitrary, assumption of risk being continuous within a category etc. (for a summary see [here](http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous)).  However, it is not within the scope of the Quality Control exercise to critique or review the chosen study design, it is mentioned only to raise awareness of the (pervasive[^2]) problem and that the review panel may critique this aspect.

Since an element of trial and error appears to have been utilised to derive optimal allocation to the three artificial categories and the risk associated with each only the final parameters used in the application have been checked.  To this end the categories, proportions and risks shown in the table below have been analysed.

Table : Categorisation, proportions and risks for the three artifical categories required based on the circumference of the cranium.

| Group Definition | Proportion | Risk |
|------------------|------------|------|
| >2SD   & < 2.5SD | 12         | 1%   |
| >2.5SD & < 3SD   | 4          | 5%   |
| >3SD             | 1          | 10%  |


```{r ordinal_regression, echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE, eval = TRUE}
## Set proportions and risk for each category
prop1 <- 12
prop2 <- 4
prop3 <- 1
risk1 <- 1
risk2 <- 1

## Function for simulating population based on specified parameters
mirabile_sim <- function(n     = 100,
                         prop1 = prop1,
                         prop2 = prop2,
                         prop3 = prop3,
                         risk1 = risk1,
                         risk2 = risk2,
                         risk3 = risk3,
                         nsim  = 10,
                         ...){
    ## Initialise results
    results <- list()
    results$simulated <- matrix(nrow = n, ncol = 3) %>% as.data.frame()
    names(results$simulated) <- c('uniform', 'group', 'event')
    test$row <- rownames(test) %>% as.numeric()
    ## Check whether the proportions sum to 1
    prop_all <- prop1 + prop2 + prop3
    ## If they don't then true poportions have not been supplied, in which
    ## case rescale
    if(prop.all != 1){
        prop1 <- prop1 / prop_all
        prop2 <- prop2 / prop_all
        prop3 <- prop3 / prop_all
    }
    ## Derive data set
    results$simulated <- results$simulated %>%
                         mutate(## Assign group classification
                                group = case_when(.$row <  (n * prop1) ~ 1,
                                                  .$row >= (n * prop1) & .$row < (n * (prop1 + prop2)) ~ 2,
                                                  .$row >= (n * (prop1 + prop2))] ~ 3),
                                ## Probability of event (i.e. abnormality) *within* group
                                event = case_when(.$group == 1 ~ rbinom(n = n, p = risk1, size = 1),
                                                  .$group == 2 ~ rbinom(n = n, p = risk2, size = 1),
                                                  .$group == 3 ~ rbinom(n = n, p = risk3, size = 1))
    ## Test for association
    results$test <- glm(event ~ group)
}

```

The resulting simulations indicate that a statistically significant result would be obtained.

One final comment on this approach is that it assumes the analyses will be conducted purely on the risk categories alone and no other information on individuals will be included.  This may be the intention but often co-variates are used in the analyses when conducted and this will increase the power to detect a difference as the variation attributable to co-variates is accounted for in the analytical process.  Thus the stated numbers are in essence a worse case scenario and including co-variates may result in an analysis that has greater power than that which has been calculated.  An example of the impact of including co-variates on simulation based sample size calculations can be found [here](http://egap.org/content/power-analysis-simulations-r).

[^2] This is not the first study to do so that the author has encountered, and it is highly unlikely to be the last, despite evidence to the contrary.
